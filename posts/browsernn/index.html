<!DOCTYPE html>
<html lang="en">
	
	<head>
		<title>browserNN.js</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width-device-width, initial-scale=1">
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
		<script type="text/x-mathjax-config">

            MathJax.Hub.Config({
            	jax: ["input/TeX", "output/HTML-CSS"],
             	extensions: ["tex2jax.js"],
             	"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
             	tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
             	TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
             	messageStyle: "none"
             });
        </script> 

		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>
		
		<script src="https://d3js.org/d3.v5.min.js"></script>
		<script src="././js/highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
		<script src="./src/utils.js"></script>
		<script src="./src/browsernn.js"></script>
		<script src="./src/index.js"></script>
		<script src="./src/classify.js"></script>
		<script src="./src/data-sets.js"></script>
	
		<style>

			.title-article {
			    grid-column: text;
			    text-align: center;
			    padding-top: 20px;
			}

			.blog-article {
			  	margin-left: auto;
			    margin-right: auto;
			    padding: 15px;
			    /*max-width: 448px;*/
			    position: relative;
			    width: 90%;
			    display:block;
			    font-family: Helvetica, Arial, sans-serif;
			}


			.blog-article h2 {
			    line-height: 1.25em;
			    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
			    padding-bottom: .5em;
			}

			.blog-article .network {
		 		margin-left:auto;
		 		margin-right:auto;
		 		display:block;
		 		width:100%;
				height: 200px;
			}

			@media (min-width: 800px) {
				.title-article {
			    grid-column: text;
			    text-align: center;
			    padding-top: 20px;
			}

			.blog-article {
			  	margin-left: auto;
			    margin-right: auto;
			    width: 748px;
			    display:block;
			    font-family: Helvetica, Arial, sans-serif;
			}


			.blog-article h2 {
			    line-height: 1.25em;
			    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
			    padding-bottom: .5em;
			}

			.blog-article .network {
		 		margin-left:auto;
		 		margin-right:auto;
		 		display:block;
		 		width:100%;
				height: 200px;
			}

			}

			.canvas-article {
				display:block;
				width: 900px;
				margin-right: auto;
				margin-left: auto;
				margin-top:-15px;
				float:center;
				font-weight: 400;
			}
			.canvas-article .pane {
			 	width: 425px;
			 	display: inline-block;
			 	padding: 15px;
			 	vertical-align: top;
			}

			.blog-article .button-refresh {
				display: block;
				margin: 0;
			}

			.canvas-article .classify-canvas {
			 	border: 1px solid rgba(0, 0, 0, 0.1);
			  	border-radius: 4px;
			 	box-shadow: 0 0 3px rgba(0, 0, 0, 0.08);
			}

			.canvas-article .rep-canvas {
			 	border: 1px solid rgba(0, 0, 0, 0.1);
			 	border-radius: 4px;
			 	box-shadow: 0 0 3px rgba(0, 0, 0, 0.08);
			}

			input[type="submit"]
			{
			 	margin: 3px;
			}

			#data-set:hover {
				border: 1px solid rgba(0, 0, 0, 0.2);
			  	opacity: 1;
			}

			#data-set {
			    margin-top:8px;
			    opacity: 0.6;
			    border: 1px solid rgba(0, 0, 0, 0.1);
			    border-radius: 4px;
			    box-shadow: 0 0 3px rgba(0, 0, 0, 0.5);
			  }

			#data-set.selected {
			    border: 2px solid rgba(70, 130, 180, 0.8);
			    opacity: 1;
			}

			pre {
				border: 0; 
				background-color: transparent;
			}

			.layer-menu #layer-dropdown {
			 	background-color: #ffffff !important;
			 	color: #3497FF !important;
			 	min-width: 200px !important;
			 	font-weight: bold;
			 	border: 1px solid rgba(0, 0, 0, 0.1);
			 	border-radius: 3px;
			 	box-shadow: 0 0 3px rgba(0, 0, 0, 0.08);  
			 	margin: 10px 0 10px 0;
			}

			#cycle-button {
  				margin: 10px;

			}

			#text-indent {
				padding-left: 3em;
			}

			.demo {
			    position: relative; /*so I could use 'top: 30px' */
			    top: 10px;
			    /*so you can see it in demo*/
			    width: 100%;
			    border-top:1px solid black;
			}

			input:-webkit-autofill {
			    -webkit-text-fill-color: yellow !important;
			}

		
		</style>
	
	</head>

	<body>

		<script src="../../js/header.js"></script>
		<link rel="stylesheet" href="./../../css/style.css">
		<style>
			.tan-header .content {
				margin-left: auto;
				margin-right: auto;
				height: 60px;
				width: 100%;
			}
			.tan-header {
				    height: 60px;

				 }
				
				.tan-header a {
					color: rgba(255, 255, 255, 0.8);
					font-size: 6px;
					text-decoration: none;
					padding: 22px 0;
				}

				.tan-header .logo {
					font-size: 22px;
					font-weight: 200;
					line-height: 60px;
					margin-left: 30px;
				}

				.tan-header .nav a  {
					font-size: 12px;
					font-weight: 200;
					line-height: 60px;
					margin-right: 10px;
				}

			@media (min-width: 748px) {

		        .tan-header {
				    height: 60px;

				 }
				.tan-header .content {
					margin-left: auto;
				    margin-right: auto;
				  	width: 786px;
				 }
				.tan-header a {
					color: rgba(255, 255, 255, 0.8);
					font-size: 6px;
					text-decoration: none;
					padding: 22px 0;
				}

				.tan-header .logo {
					font-size: 22px;
					font-weight: 200;
					line-height: 60px;
					margin-left: 30px;
				}

				.tan-header .nav a  {
					font-size: 14px;
					font-weight: 200;
					line-height: 60px;
					margin-right: 10px;
				}
			}
		</style>

		<div class="title-article">		

       		<h1>browserNN.js</h1>
       		<p>Train neural networks in your browser!</p>
       		<p><em>Aug. 5, 2018</em></p>

       		<hr>

       	</div>

   		<div class="blog-article">

       		<h2>Introduction</h2>

			Inspired by Keras, Andrej Karpathy, and dynet, browserNN.js allows you to train deep neural networks in your own browser. The demo below offers various 2d datasets (left canvas) to train nueral networks on. The canvas on the right visualizes the transformed feature space of 2 neurons at a given layer in the network. You can make changes to the code below to train different networks and hyperparameters. You can also choose to visualize different layers using the dropdown box under the right canvas. Documentation on how the library works is below. Have fun!
			</p>

			<h2>Demo</h2>
			<br>
			<textarea class="network"></textarea><br>
			<input class="button-refresh" type="submit" value="Retrain model" onclick="refresh();"><br>

		</div>
			
		<div class="canvas-article">				
			
			<div class="panes">

				<div class="pane">
					<canvas class="classify-canvas" width="400" height="400">Web browser not supported. Try google chrome :)</canvas>
					<div class="row">
						<div id="data-options">
						</div>
					</div>
				</div>

				<div class="pane">
					<canvas class="rep-canvas" width="400" height="400">Web browser not supported. Try google chrome :)</canvas>
					<div id="cycle-weights"></div>
					<div class="layer-menu"></div>
				</div>

			</div>
		</div>

		<div class="blog-article">
			
			<h3>Overview</h3>
			<hr>
			<p>
				
				The <font color="#C86464"><b>red</b></font> and <font color="#04B7D6"><b>blue</b></font> points are used to train the neural network, while the rest of the pixels in left canvas are used to predict which class they should belong to. Every <!-- $\rfrac{1}{10}$ $^1/_{10}$  --> $^1/_{10}$ of a second the network updates it's weights and makes new predictions on data from the left canvas. The canvas on the right is a direct visual of what two neurons look like at a given layer in the network. For example, the demo is defaulted to the first two neurons of the $2^{nd}$ layer in the network after applying a tanh activation function. The neurons $\boldsymbol{h}^{(2)}_0$ and $\boldsymbol{h}^{(2)}_1$ come from the hidden layer, $\boldsymbol{h}^{(2)}=f(\boldsymbol{W}^{(2)}\boldsymbol{h}^{(1)}+b^{(2)})$, where $h^{(l)}$ is the output vector of layer $l$, $\boldsymbol{W}^{(l)}$ is the weight matrix from layer $l$, $b^{(l)}$ is the layers bias parameter, and $f$ is the tanh function. The <font color="#C86464"><b>red</b></font> and <font color="#04B7D6"><b>blue</b></font> points lie on a 2-dimensional grid where each line represents a strip of pixels that run along the x or y-axis of the left canvas. These lines help us see how the feature space stretches and bends in each layer of the neural network. 

			</p>


			<h3>Dense Layer</h3>
			<hr>
			<p>
				Every <code>dense</code> layer is decomposed into two seperate layers: 
				<ol>
					<li>
						The linear transformation from the previous layer to the current.
						<ul>
							<li>
								$\boldsymbol{z}^{(l)}=\boldsymbol{W}^{(l)} \boldsymbol{h}^{(l-1)}+b^{(l)}$
							</li>
						</ul>
					</li>
					<li>
						The hidden output after applying an activation function to the linear transformation. 
						<ul>
							<li>
								$\boldsymbol{h}^{(l)}=f(\boldsymbol{z}^{(l)})=f(\boldsymbol{W^{(l)}}\boldsymbol{h}^{(l-1)}+b^{(l)})$
							</li>
						</ul>
					</li>
				</ol>
				This allows us to see the before and after effects of applying activation functions.
			</p>

			<h3>Output</h3>
			<hr>
			<p>
				If you select the $1^{st}$ layer to visualize you'll notice that it looks no different from the data in the left canvas. This is simply because the network has not yet applied a nonlinear activation function. Once we apply at least one nonlinear function to our data we'll begin to see the data stetch and bend in the network. 
				<br><br>	
				Now if we select the <font color="#3497FF"><b>Output</b></font> layer you'll notice that if the model is able to classify the data correctly on the left - it has actually pulled the <font color="#C86464"><b>red</b></font> and <font color="#04B7D6"><b>blue</b></font> points completely apart from each other on the right  to the point where you could draw one straight line in between them to accurately classify the two groups. This is the beauty of what neural networks are trying to accomplish :)
			</p>
			
		</div>


		<hr>
		
		<div class="blog-article">
					
			<h2>Documentation</h2>
		</div>

		<div class="section-article">

		
		</div>

		<div class="blog-article">			
					
			<h3>Tensor</h3>
			<hr>

			<p>
				<code>Tensor</code> is the building block of the library. It creates a 3 dimensional matrix and stores it's weights, derivatives, and dimension details. All inputs, weight matrices, and outputs must all be a Tensor class. 
			</p>

			<h4>Example</h4>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">tensor = new browsernn.Tensor(1, 2, 1, 0.0); // 1x2x1 tensor initialized with zeros
tensor = new browsernn.Tensor(28, 28, 3); // 28x28x3 tensor with randomly initialized values

var data = [[10,20,30],[40,50,60]];
tensor = new browsernn.Tensor(2, 3, 1, data); // 2x3x1 tensor initialized with data values

tensor.w[4]; // returns 50
tensor.dw[4]; // returns 0 since no derivative has been computed yet</code></pre>

			<h4>Arguments</h4>
			<ul>
				<li>
					<b>in_n</b>: number of rows
				</li>
				<li>
					<b>in_d</b>: number of cols
				</li>
				<li>
					<b>in_depth</b>: depth of the tensor
				</li>
				<li>
					<b>init_weight</b>: constant or array of weights to initialize each value in the tensor
				</li>
				<li>
					<b>seed</b>: integer to use as random seed
				</li>
			</ul>

			<h4>Properties</h4>

			The object <code>Tensor</code> returns has the following properties:

			<ul>
				<li>
					<b>n</b>: number of rows
				</li>
				<li>
					<b>d</b>: number of cols
				</li>
				<li>
					<b>depth</b>: depth of the tensor
				</li>
				<li>
					<b>n_cells</b>: Total number of values the tensor holds. <code>n_cells</code> = <code>n</code> * <code>d</code> * <code>depth</code>
				</li>
				<li>
					<b>w</b>: Array of length <code>n_cells</code> that holds the weights for the tensor. 
				</li>
				<li>
					<b>dw</b>: Array of length <code>n_cells</code> that holds the derivates for the tensor. 
				</li>
			</ul>

			<h3>Model</h3>
			<hr>
			<p><code>Model()</code> is used to instantiate a browserNN network.</p>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">model = new browsernn.Model();</code></pre>
			
			<h3>Layers</h3>
			<hr>

			<p>
				Each layer must be passed into a list as an object containing a set of parameters to define the layer. Each network must start with an <code>input</code> layer and end with a loss layer like <code>softmax</code>, <code>SVM</code>, etc.
			</p>

			<h4>Example</h4>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">layers = [];

// multilayer perceptron
layers.push({type: 'input', in_n: 1, in_d: 2, in_depth: 1});
layers.push({type: 'dense', n_neurons: 8, activation: 'relu'});
layers.push({type: 'dense', n_neurons: 4, activation: 'relu'});
layers.push({type: 'softmax', n_classes: 2});</code></pre> 

			<h4>Arguments</h4>
			<ul>
				<li>
					<b>type</b>: Name of the layer 
				</li>
				<li>
					<b>in_n</b> (input): Number of rows for an input layer
				</li>
				<li>
					<b>in_d</b> (input): Number of cols for an input layer
				</li>
				<li>
					<b>in_depth</b> (input): Depth of an input layer
				</li>
				<li>
					<b>n_neurons</b> (dense): Number of units in a hidden layer
				</li>
				<li>
					<b>activation</b> (dense): Name of the activation function applied element-wise
				</li>
				<li>
					<b>n_classes</b> (output): Number of classes to predict on
				</li>
			</ul>

			<h3>Layer types</h3>
			<hr>
			
			<h4>Input</h4>
			<p>
				The <code>input</code> layer must always be defined at the start of a network, which indicates what the expected input shape will be when passed into the model. If no parameter for <code>in_depth</code> is passed the model will assume tensor of size $(n, d, 1)$. 
			</p>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">layers.push({type = 'input', in_n: 28, in_d: 28}); // 28x28 input matrix</code></pre>
			
			<h4>Dense</h4>
			<p>
				The <code>dense</code> layer acts as a densly-connected layer that returns the output $\boldsymbol{h} = f(\boldsymbol{W} \cdot \boldsymbol{x} + b)$ where $\boldsymbol{W}$ is the weight matrix created by the current layer, $\boldsymbol{x}$ is the input vector from the previous layer, $b$ is the bias parameter created by the current layer, and $f$ is the <code>activation</code> function. The activation functions currently supported are: <code>linear</code>, <code>sigmoid</code>, <code>relu</code>, and <code>tanh</code>.

			</p>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">layers = [];
layers.push({type = 'dense', n_neurons: 64, activation: 'sigmoid'});
layers.push({type = 'dense', n_neurons: 64, activation: 'relu'});
layers.push({type = 'dense', n_neurons: 64, activation: 'tanh'});
layers.push({type = 'dense', n_neurons: 64, activation: 'linear'});</code></pre>
			
			<h4>Softmax</h4>
			<p>
				The last layer of each network must be a loss layer, one of which can be <code>softmax</code>. For classification the softmax layer outputs a probability for each of the N number of classes defined by <code>n_classes</code> and predicts the class with the highest probability. 
			</p>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">layers.push({type: 'softmax', n_classes: 10}); // predicts among 10 classes</code></pre>

			<h4>SVM</h4>
			<p>
				Similar to the <code>softmax</code> layer, <code>SVM</code> creates a Support Vector Machine layer that outputs scores for the <code>n_classes</code> passed through. 
			</p>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">layers.push({type: 'SVM', n_classes: 2});</code></pre>

			<h3>Trainers</h3>
			<hr>

			<p>
				<code>Trainer()</code> is the last argument needed to train a model. This class takes in the initialized <code>model</code>, <code>layers</code>, and <code>parameters</code> and begins training once the method <code>.fit()</code> is called. The optimizers currently supported are <code>SGD</code>, <code>Adagrad</code>, <code>Adadelta</code>, and <code>Adam</code>. 
			</p>

			<h4>Example</h4>
			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">trainer = new browsernn.Trainer(model, layers, params);</code></pre>

			<h4>Arguments common to all optimizers</h4>
			<ul>
				<li>
					<b>batch_size</b>: Number of samples per gradient update. Defaults to 1.
				</li>
				<li>
					<b>seed</b>: seed: integer to use as random seed.
				</li>
			</ul>

			<h4><b>SGD</b></h4>
			<p>
				Stochastic Gradient Descent optimizer with support for momentum. 
			</p>

			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">params = {
	optimizer: 'SGD',
	learning_rate: 0.01,
	momentum: 0.9,ls
	l2_decay: 0.01,
	batch_size: 10
};
trainer = new browsernn.Trainer(model, layers, params);</code></pre>
			
			<h4>Arguments</h4>

			<ul>
				<li>
					<b>learning_rate</b>: Learning rate, $\geq$ 0.
				</li>
				<li>
					<b>momentum</b>: Parameter used to accelerate SGD by smoothing out gradients and dampening oscillations.
					<div id="text-indent">
						<b>Note</b>: It is typically recommended to use 0.5, 0.9, or 0.99 for <code>momentum</code> with variations of the learning rate, however, for the 2D datasets above a small momentum of 0.1 is typically sufficient, since the gradients will be relatively small.
					</div>
				</li>
				<li>
					<b>l2_decay</b>: $L_2$ weight penalty. Regularizer that shrink weights heavily when their values are high, which typically leads to slower convergence, but stronger generalization.
					<div id="text-indent">
						<b>Note</b>: Try different values of <code>l2_decay</code> in the range 0.0001 - 0.5 to see it's effect on the models ability to learn.
					</div>
				</li>
				<li>
					<b>l1_decay</b>: $L_1$ weight penalty.
				</li>
			</ul>

			<hr>

			<h4><b>Adagrad</b></h4>
			<p>Adagrad is an optimizer that adapts learning rates based on how frequently a parameter is changed while training. It is similar to SGD with momentum, however, shrinks it's learning rate over time by accumulating all historical gradients at each step.</p>

			<h4>Example</h4>
			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">params = {
	optimizer: 'adagrad',
	learning_rate: 0.01,
	epsilon: 1e-7,
	batch_size: 10
};
trainer = new browsernn.Trainer(model, layers, params);</code></pre>
	
			<h4>Arguments</h4>
			<ul>
				<li>
					<b>learning_rate</b>: Initial learning rate adapted over time. 
				</li>
				<li>
					<b>epsilon</b>: Parameter used to smooth gradients and prevent division by zero. Defaults to $10^{-7}$. 
				</li>
			</ul>


			<h4><b>Adadelta</b></h4>

			<p>
				Adadelta is an extension of Adagrad, which scales the learning rate based off of recently accumulated gradients instead of all historical gradients like the way <code>adagrad</code> does. This prevents the learning rate from converging to zero, which typically happens after long periods of training with Adagrad. 
			</p>

			<h4>Example</h4>
			<pre style="border: solid 1px #e1e4e5; padding: 2px; background-color:#FFFBFB;"><code class="javascript hljs">params = {
	optimizer: 'adadelta',
	learning_rate: 1.0,
	epsilon: 1e-7,
	rho: 0.95,
	batch_size: 10
};
trainer = new browsernn.Trainer(model, layers, params);</code></pre>

			<h4>Arguments</h4>
			<ul>
				<li>
					<b>learning_rate</b>: Initial learning rate adapted over time. Defaults to 1.
					<div id="text-indent">
						<b>Note</b>: This parameter should be left at 1. 
					</div>
				</li>
				<li>
					<b>epsilon</b>: Parameter used to smooth gradients and prevent division by zero. Defaults to $10^{-7}$. 
				</li>
				<li>
					<b>rho</b>: Decay factor, corresponding to fraction of gradients to accumulate at each time step. Recommended to use 0.95. 
				</li>
			</ul>

		</div>
		<hr>

		<div class="blog-article">

			<h2>Conclusion</h2>
			<p>
				This library is simply for educational purposes. It is in no way intended for production use, but rather for myself and others to explore how different hyperparameters and optimization methods effect neural networks at a low level - hence this 2D demo. 
			</p>

			<h3>Source</h3>
			<hr>

			<p>
				For full source code please see: <a href="https://github.com/carbonati/browsernn">https://github.com/carbonati/browsernn</a>
			</p>
		</div>

		<br><br><br>
	</body>
</html>